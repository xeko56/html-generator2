{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8256dda-bb1a-4750-b0c6-3f89bb088b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff5f461-5685-4e8e-b580-9942c5591489",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch-lightning wandb torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c51d7290-f200-48f2-a0c7-52b216b90cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "image_size = [1280, 960]\n",
    "max_length = 512\n",
    "\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "config.encoder.image_size = image_size\n",
    "config.decoder.max_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc333c1a-28d2-44fa-ba31-23744ebf1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1a39b50-d30b-4d16-a099-2af8af03740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_html(html):\n",
    "    # Replace spaces within style attributes with a non-breaking space\n",
    "    html = re.sub(r'(style=\"[^\"]+\")', lambda x: x.group(1).replace(' ', '\\xa0'), html)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f6966a0-8c5c-443e-b17a-1ec6efc188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class HtmlTablesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_file: str,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"\",\n",
    "        prompt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        with open(json_file, 'r') as file:\n",
    "            # Split data\n",
    "            data = json.load(file)\n",
    "            total_len = len(data)\n",
    "            train_end = int(0.7 * total_len)\n",
    "            val_end = int(0.85 * total_len)\n",
    "            \n",
    "            if self.split == 'train':\n",
    "                self.data_pairs = data[:train_end]\n",
    "            elif self.split == 'validation':\n",
    "                self.data_pairs = data[train_end:val_end]\n",
    "            elif self.split == 'test':\n",
    "                self.data_pairs = data[val_end:]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid split name\")             \n",
    "        self.dataset_length = len(self.data_pairs)       \n",
    "\n",
    "        # Initialize transformations for images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        html_tokens = ['<s_html>', '<table>', '<table style=\"border-collapse: collapse;\">', '<th>'\n",
    "                       , '<th style=\"border: 1px solid black;\">', '<tr>', '<td>', '</td>'\n",
    "                       , '<td style=\"border: 1px solid black;\">', '</tr>', '</th>', '</table>, </s_html>']\n",
    "        self.add_tokens(html_tokens)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.data_pairs:\n",
    "            gt_jsons = sample[\"html\"]\n",
    "            self.gt_token_sequences.append(self.minify_html(gt_jsons))\n",
    "       \n",
    "        if task_start_token or prompt_end_token:\n",
    "            # Assuming the tokenizer can handle adding tokens if necessary\n",
    "            self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
    "            self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def minify_html(self, html: str):\n",
    "        # Replace escaped double quotes with regular double quotes\n",
    "        html = html.replace('\\\\\"', '\"')\n",
    "        # Remove newline characters\n",
    "        html = html.replace('\\n', '')\n",
    "        # Optionally, remove extra spaces between tags if they exist\n",
    "        html = ' '.join(html.split())\n",
    "        return html\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"\", fr\"\"])\n",
    "                    output += (\n",
    "                        fr\"\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "    \n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def reassemble_html_tokens(self, tokens):\n",
    "        # Implement reassembly logic that was discussed previously\n",
    "        new_tokens = []\n",
    "        buffer = \"\"\n",
    "        for token in tokens:\n",
    "            if token.startswith(\"▁\") and buffer:\n",
    "                new_tokens.append(buffer)\n",
    "                buffer = token[1:]  # Remove the '▁' for a new token\n",
    "            else:\n",
    "                buffer += token.replace(\"▁\", \"\")  # Remove '▁' and append to the current buffer\n",
    "        if buffer:\n",
    "            new_tokens.append(buffer)  # Append the last buffer if any\n",
    "        return new_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_pairs[idx]\n",
    "        image_path = item['image'].replace(\"\\\\\", \"/\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])\n",
    "        html_content = item['html']\n",
    "        encoded_html = processor.tokenizer(\n",
    "            html_content,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = encoded_html.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id \n",
    "    \n",
    "        return image, labels, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bc59c42-2a14-4527-94f1-828d863e5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor.size = image_size[::-1]\n",
    "processor.image_processor.do_align_long_axis = False\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Initialize the dataset\n",
    "\n",
    "train_dataset = HtmlTablesDataset(\n",
    "    json_file='./data_pairs.json', \n",
    "    max_length=2056,                         \n",
    "    split=\"train\", \n",
    "    task_start_token=\"\", \n",
    "    prompt_end_token=\"\",\n",
    "    ignore_id=-100,\n",
    ")\n",
    "\n",
    "val_dataset = HtmlTablesDataset(\n",
    "    json_file='./data_pairs.json',\n",
    "    max_length=2056,                         \n",
    "    split=\"validation\", \n",
    "    task_start_token=\"\", \n",
    "    prompt_end_token=\"\",\n",
    "    ignore_id=-100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50245adf-9cb6-49b3-82eb-37a35ee6f699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5a2bdb4-c26a-4105-a578-1d9a3ca2cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='naver-clova-ix/donut-base', vocab_size=57522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s_iitcdip>', '<s_synthdog>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57521: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t57522: AddedToken(\"<sep/>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57523: AddedToken(\"<s_iitcdip>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57524: AddedToken(\"<s_synthdog>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t57525: AddedToken(\"<s_html>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57526: AddedToken(\"<table>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57527: AddedToken(\"<table style=\"border-collapse: collapse;\">\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57528: AddedToken(\"<th>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57529: AddedToken(\"<th style=\"border: 1px solid black;\">\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57530: AddedToken(\"<tr>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57531: AddedToken(\"<td>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57532: AddedToken(\"</td>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57533: AddedToken(\"<td style=\"border: 1px solid black;\">\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57534: AddedToken(\"</tr>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57535: AddedToken(\"</th>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t57536: AddedToken(\"</table>, </s_html>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df7440f-8482-46fb-8fa9-666d03ad1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of tokens: 57522\n",
      "Number of tokens after adding special tokens: 57537\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of tokens:\", processor.tokenizer.vocab_size)\n",
    "print(\"Number of tokens after adding special tokens:\", len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b49e411-f405-4264-84dd-64c61a675bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values, labels, target_sequence = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ad2145b-c437-4f4a-a721-162da9a6b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5136ddb-6a45-43af-9907-50b6c15c7409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "<table style=\"border-collapse: collapse;\">\n",
      "\n",
      "<tr>\n",
      "\n",
      "<th style=\"border: 1px solid black;\">\n",
      "Tas\n",
      "k\n",
      "</th>\n",
      "\n",
      "<th style=\"border: 1px solid black;\">\n",
      "As\n",
      "sign\n",
      "ed\n",
      "To\n",
      "</th>\n",
      "\n",
      "<th style=\"border: 1px solid black;\">\n",
      "Du\n",
      "e\n",
      "Date\n",
      "</th>\n",
      "\n",
      "</tr>\n",
      "\n",
      "<tr>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Design\n",
      "Home\n",
      "page\n",
      "</td>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Alice\n",
      "</td>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "20\n",
      "23\n",
      "-11\n",
      "-15\n",
      "</td>\n",
      "\n",
      "</tr>\n",
      "\n",
      "<tr>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Develop\n",
      "Back\n",
      "end\n",
      "API\n",
      "</td>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Bob\n",
      "</td>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "20\n",
      "23\n",
      "-11\n",
      "-20\n",
      "</td>\n",
      "\n",
      "</tr>\n",
      "\n",
      "<tr>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Set\n",
      "up\n",
      "Data\n",
      "base\n",
      "</td>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Charlie\n",
      "</td>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "20\n",
      "23\n",
      "-11\n",
      "-18\n",
      "</td>\n",
      "\n",
      "</tr>\n",
      "\n",
      "<tr>\n",
      "\n",
      "<td style=\"border: 1px solid black;\">\n",
      "Con\n",
      "duct\n",
      "User\n",
      "Test\n",
      "ing\n",
      "</td>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id in labels.tolist()[:100]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f01575b-f1b8-4b25-a578-84936dfa11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_html>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "390b6512-4a07-4bff-90c9-39e01796864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token ID: <pad>\n",
      "Decoder start token ID: <s_html>\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601e1e2f-d430-4909-a7a2-0f39e2371d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bb221-cc1d-4129-932d-181cf5f66ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8241b728-be90-4458-b9bb-c911ecad014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56938ee3-c0ea-4ba0-adde-170bd202067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c9c42cd-522b-439e-9777-5bd5aeb2f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, labels, _ = batch\n",
    "        \n",
    "        outputs = self.model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, labels, answers = batch\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        # we feed the prompt to the model\n",
    "        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n",
    "        \n",
    "        outputs = self.model.generate(pixel_values,\n",
    "                                   decoder_input_ids=decoder_input_ids,\n",
    "                                   max_length=max_length,\n",
    "                                   early_stopping=True,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=1,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "    \n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "            predictions.append(seq)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=\", \"\", answer, count=1)\n",
    "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "    \n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88004d0b-c76c-4378-889b-6df5f9250c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\":30,\n",
    "          \"val_check_interval\":0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\":1,\n",
    "          \"gradient_clip_val\":1.0,\n",
    "          \"num_training_samples_per_epoch\": 800,\n",
    "          \"lr\":3e-5,\n",
    "          \"train_batch_sizes\": [8],\n",
    "          \"val_batch_sizes\": [1],\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 300, # 800/8*30/10, 10%\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "          }\n",
    "\n",
    "model_module = DonutModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a8ee1-3ca5-4474-9bfc-75ff0cd27144",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
